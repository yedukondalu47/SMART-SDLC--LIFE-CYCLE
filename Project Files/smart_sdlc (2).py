# -*- coding: utf-8 -*-
"""Smart-sdlc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XgvmXL0ElMGpPVqVRHMe7adu_oPDT5G4
"""

!pip install transformers torch gradio accelerate bitsandbytes PyPDF2

# Imports
import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import PyPDF2

# SmartSDLC-AI Core Class
class SmartSDLC_AI:
    def __init__(self):
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self.load_model()

    def load_model(self):
        try:
            print("üîÑ Loading AI model...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
            )
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=1024,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            print("‚úÖ AI model loaded.")
        except Exception as e:
            print(f"‚ùå Error: {e}")
            print("‚ö†Ô∏è Falling back to DialoGPT-medium...")
            fallback_model = "microsoft/DialoGPT-medium"
            self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)
            self.model = AutoModelForCausalLM.from_pretrained(fallback_model)
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=1024,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            print("‚úÖ Fallback model loaded.")

    def analyze_requirements(self, text):
        prompt = f"You are a software requirement analysis assistant. Analyze the following requirements and list key functionalities, ambiguities, and improvement suggestions:\n\n{text}\n\nResponse:"
        response = self.pipeline(prompt)
        result = response[0]['generated_text'].split("Response:")[-1].strip()
        return result

    def generate_code(self, description):
        prompt = f"You are a software code generation assistant. Based on the following description, generate Python code:\n\n{description}\n\nCode:"
        response = self.pipeline(prompt)
        result = response[0]['generated_text'].split("Code:")[-1].strip()
        return result

# üìë PDF Text Extraction Function
def extract_text_from_pdf(file_obj):
    reader = PyPDF2.PdfReader(file_obj)
    text = ""
    for page in reader.pages:
        text += page.extract_text() or ""
    return text

# üñ•Ô∏è Gradio Interface Builder
def create_gradio_interface():
    with gr.Blocks(title="SmartSDLC-AI") as app:
        gr.HTML("<h1 style='text-align:center;'>üõ†Ô∏è SmartSDLC-AI</h1><p style='text-align:center;'>AI-powered Requirement Analysis & Code Generation Assistant</p>")

        with gr.Tabs():
            # üìÑ Requirement Analysis Tab
            with gr.Tab("üìÑ Requirement Analysis"):
                with gr.Row():
                    pdf_input = gr.File(label="Upload PDF Requirements")
                    text_input = gr.Textbox(label="Or Enter Requirements Prompt", lines=6)
                analyze_btn = gr.Button("Analyze Requirements")
                analysis_output = gr.Textbox(label="Requirement Analysis Result", lines=12)

            # üíª Code Generation Tab
            with gr.Tab("üíª Code Generation"):
                code_desc_input = gr.Textbox(label="Describe the Functionality for Code Generation", lines=6)
                generate_code_btn = gr.Button("Generate Code")
                code_output = gr.Code(label="Generated Python Code", language="python")

        # Requirement Analysis Function
        def handle_analysis(pdf_file, prompt_text):
            if pdf_file:
                text = extract_text_from_pdf(pdf_file)
            elif prompt_text.strip():
                text = prompt_text
            else:
                return "‚ùó Please upload a PDF or enter requirement text."
            result = smart_sdlc_ai.analyze_requirements(text)
            return result

        analyze_btn.click(fn=handle_analysis, inputs=[pdf_input, text_input], outputs=analysis_output)

        # Code Generation Function
        def handle_code_generation(desc):
            if not desc.strip():
                return "‚ùó Please enter a description for code generation."
            result = smart_sdlc_ai.generate_code(desc)
            return result

        generate_code_btn.click(fn=handle_code_generation, inputs=code_desc_input, outputs=code_output)

        gr.HTML("<p style='text-align:center; color:gray;'>‚öôÔ∏è Powered by IBM Granite AI | SmartSDLC-AI for Modern Development</p>")

    return app

# üöÄ Run Application
if __name__ == "__main__":
    print("üöÄ SmartSDLC-AI Initializing...")
    smart_sdlc_ai = SmartSDLC_AI()
    iface = create_gradio_interface()
    print("üåê Launching SmartSDLC-AI with public link...")
    iface.launch(share=True)